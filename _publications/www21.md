---
title: "AID: Active Distillation Machine to Leverage Pre-TrainedBlack-Box Models in Private Data Settings"
collection: publications
permalink: /publication/www21
excerpt: 'Trong Nghia Hoang, Shenda Hong, Cao Xiao, Kian Hsiang Low and Jimeng Sun'
date: 2021-04-19
venue: '30th The Web Conference (WWW)'
---
Abstract: This paper presents an active distillation method for a local institution (e.g., hospital) to find the best queries within its given budget to distill an on-server black-box model's predictive knowledge into a local surrogate with transparent parameterization. This allows local institutions to understand better the predictive reasoning of the black-box model in its own local context or to further customize the distilled knowledge with its private dataset that cannot be centralized and fed into the server model. The proposed method thus addresses several challenges of deploying machine learning in many industrial settings (e.g., healthcare analytics) with strong proprietary constraints. These include: (1) the opaqueness of the server modelâ€™s architecture which prevents local users from understanding its predictive reasoning in their local data contexts; (2) the increasing cost and risk of uploading local data on the cloud for analysis; and (3) the need to customize the server model with private onsite data. We evaluated the proposed method on both benchmark and real-world healthcare data where significant improvements over existing local distillation methods were observed. A theoretical analysis of the proposed method is also presented.

[Paper](http://htnghia87.github.io/files/www21.pdf)
[Bibtex](http://htnghia87.github.io/files/www21.bib)